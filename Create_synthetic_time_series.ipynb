{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kramersmoyal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kramersmoyal import km\n",
    "\n",
    "import os\n",
    "   \n",
    "\n",
    "from utils.Data_cleaning import data_cleaning\n",
    "from utils.Functions import data_filter, integrate_omega, KM_Coeff_1, KM_Coeff_2, daily_profile, power_mismatch, exp_decay, Euler_Maruyama, Increments, autocor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the grid: (Balearic, Irish or Iceland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grids = ['Iceland','Irish','Balearic']    \n",
    "grids = ['Balearic']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of too high frequency values:  0 Number of too low frequency values:  0\n",
      "Number of isolated peaks:  0\n",
      "Number of windows with constant frequency for longer than 15s:  0\n",
      "Number of Nan-intervals:  0\n",
      "Clean corrupted data ...\n"
     ]
    }
   ],
   "source": [
    "models = ['model 1','model 2','model 3','model 4']\n",
    "\n",
    "#freq_orig = data/(2*np.pi+50)\n",
    "#increments_orig = Increments(data/(2*np.pi+50))\n",
    "\n",
    "'''For calculations: use angular velocity omega = 2*pi*frequency '''\n",
    "'''The bandwidth is chosen such that we receive a scmooth distribution'''\n",
    "\n",
    "'''Choose the grid '''\n",
    "\n",
    "'''Data analysis of the original time series'''\n",
    "data_orig          = {i:[]for i in grids}\n",
    "increments_orig = {i:[]for i in grids}\n",
    "autocor_orig    = {i:[]for i in grids}\n",
    "\n",
    "\n",
    "edges_1d     = {i:[]for i in grids}\n",
    "drift_1d     = {i:[]for i in grids}\n",
    "diffusion_1d = {i:[]for i in grids}\n",
    "edges_2d     = {i:[]for i in grids}\n",
    "kmc_2d       = {i:[]for i in grids}\n",
    "\n",
    "for grid in grids:\n",
    "  time_res = 1\n",
    "  '''Choose the grid '''\n",
    "  raw=pd.read_csv('./Data/Frequency_data_%s.csv'%(grid), sep=',')\n",
    "  freq = (raw[['Frequency']]/1000 +50).squeeze()\n",
    "  freq = data_cleaning(freq)\n",
    "\n",
    "  \n",
    "  data_orig[grid].append(freq)\n",
    "  increments_orig[grid].append(Increments(freq,time_res = time_res,step = 1))\n",
    "  autocor_orig[grid].append(autocor(freq,steps = 10, time_res = time_res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical data is given by data_orig[grid] (grid = Irish, Iceland, Balearic.)\n",
    "In the following the different models are calculated, i.e. the synthetic time series is given by \"omega_synth_model_i\" (i=1,2,3,4).\n",
    "\\\n",
    "The default length of the calculated synthetic time series is 5 days (t_final=5)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of too high frequency values:  0 Number of too low frequency values:  0\n",
      "Number of isolated peaks:  0\n",
      "Number of windows with constant frequency for longer than 15s:  0\n",
      "Number of Nan-intervals:  0\n",
      "Clean corrupted data ...\n"
     ]
    }
   ],
   "source": [
    "'''Model 1...'''\n",
    "synth_data_model_1 = {i:[]for i in grids}\n",
    "increments_model_1 = {i:[]for i in grids}\n",
    "autocor_model_1 = {i:[]for i in grids}\n",
    "c_1_model1 = {i:[]for i in grids}\n",
    "epsilon_model1 = {i:[]for i in grids}\n",
    "'''adapt the parameter estimation to the particulat grids'''\n",
    "\n",
    "for grid in grids:\n",
    "  raw=pd.read_csv('./Data/Frequency_data_%s.csv'%(grid), sep=',')\n",
    "  freq = (raw[['Frequency']]/1000 +50).squeeze()\n",
    "  freq = data_cleaning(freq)\n",
    "  data = (freq-50)*(2*np.pi)   #Use the angular velocity for the calcualltions\n",
    "\n",
    "  \n",
    "  bw_drift = 0.1\n",
    "  bw_diff = 0.1\n",
    "  dist_drift = 500 #for small amount of data choose a larger value for dist_drift: 800\n",
    "  dist_diff = 500\n",
    "  \n",
    "  c_1 = KM_Coeff_1(data,dim= 1,time_res = 1,bandwidth = bw_drift,dist = dist_drift, order = 1)\n",
    "  \n",
    "\n",
    "  epsilon = KM_Coeff_2(data,dim = 1,time_res = 1,bandwidth = bw_diff,dist = dist_diff,multiplicative_noise = False)\n",
    "  \n",
    "  c_1_model1[grid] = c_1\n",
    "  epsilon_model1[grid] = epsilon\n",
    "  \n",
    "  delta_t = 0.1\n",
    "  omega_synth_model_1 = Euler_Maruyama(data,c_1=c_1,c_2_decay=0,Delta_P = 0,epsilon=epsilon,time_res = 1,dispatch = 0,delta_t=delta_t,t_final=5,model=1)\n",
    "  \n",
    "  freq_synth_model_1 = omega_synth_model_1/(2*np.pi) + 50\n",
    "  #(data,c_1,c_2,Delta_P,epsilon,c_1_weight = 3,time_res = 1,dispatch = 1,delta_t=0.1,t_final=5,model=3,factor_daily_profile=0)\n",
    "  synth_data_model_1[grid].append(freq_synth_model_1)\n",
    "  increments_model_1[grid].append(Increments(freq_synth_model_1,time_res = delta_t,step = 1))\n",
    "  autocor_model_1[grid].append(autocor(freq_synth_model_1,steps = 10, time_res = delta_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of too high frequency values:  0 Number of too low frequency values:  0\n",
      "Number of isolated peaks:  0\n",
      "Number of windows with constant frequency for longer than 15s:  0\n",
      "Number of Nan-intervals:  0\n",
      "Clean corrupted data ...\n"
     ]
    }
   ],
   "source": [
    "'''Model 2...'''\n",
    "synth_data_model_2 = {i:[]for i in grids}\n",
    "increments_model_2 = {i:[]for i in grids}\n",
    "autocor_model_2 = {i:[]for i in grids}\n",
    "c_1_model2 = {i:[]for i in grids}\n",
    "epsilon_model2 = {i:[]for i in grids}\n",
    "\n",
    "'''adapt the parameter estimation to the particulat grids'''\n",
    "for grid in grids:\n",
    "    \n",
    "  raw=pd.read_csv('./Data/Frequency_data_%s.csv'%(grid), sep=',')\n",
    "  freq = (raw[['Frequency']]/1000 +50).squeeze()\n",
    "  freq = data_cleaning(freq)\n",
    "  data = (freq-50)*(2*np.pi)   #Use the angular velocity for the calcualltions\n",
    "    \n",
    "  trend = 1 #trend is boolean\n",
    "  bw_drift = 0.1\n",
    "  bw_diff = 0.1\n",
    "  dist_drift = 500    #for large data set: dist_drift = 350 for Balearic\n",
    "  dist_diff = 500\n",
    "  if grid == 'Balearic':\n",
    "      Delta_P = power_mismatch(data,avg_for_each_hour = False,dispatch=2,start_minute=0,end_minute=1/6,length_seconds_of_interval=5)\n",
    "      dispatch = 1\n",
    "  elif grid == 'Irish':\n",
    "      Delta_P = power_mismatch(data_filter(data,sigma = 6),avg_for_each_hour = False,dispatch=1,start_minute=0,end_minute=1/6,length_seconds_of_interval=5)\n",
    "      dispatch = 2\n",
    "      #we use a filter for the power mismatch of the Iroish data because of regular outliers (every 60 seconds)\n",
    "  elif grid == 'Iceland':\n",
    "      Delta_P = 0\n",
    "      dispatch = 0\n",
    "      trend = 0 # Represents a no-existing trend as there is no power dispatch schedule\n",
    "\n",
    "  c_1 = KM_Coeff_1(data - trend*data_filter(data),dim= 1,time_res = 1,bandwidth = bw_drift,dist = dist_drift, order = 1)\n",
    "  c_2_decay = trend*exp_decay(data,time_res=1,size = 899)\n",
    "  epsilon =   epsilon = KM_Coeff_2(data - trend*data_filter(data),dim = 1,time_res = 1,bandwidth = bw_diff,dist = dist_diff,multiplicative_noise = False)\n",
    "  \n",
    "  kmc,edges = km(data - trend * data_filter(data),powers = [0,1,2],bins = np.array([6000]),bw=bw_drift)\n",
    "  edges_1d[grid] = edges[0]\n",
    "  drift_1d[grid] = kmc[1]\n",
    "  diffusion_1d[grid] = kmc[2] \n",
    "  c_1_model2[grid] = c_1\n",
    "  epsilon_model2[grid] = epsilon\n",
    "\n",
    "  delta_t = 0.1 \n",
    "  omega_synth_model_2 = Euler_Maruyama(data,c_1=c_1,c_2_decay=c_2_decay,Delta_P = Delta_P,epsilon=epsilon,time_res = 1,dispatch = dispatch,delta_t=delta_t,t_final=5,model=2,factor_daily_profile=0)\n",
    "  \n",
    "  #(data,c_1,c_2_decay,Delta_P,epsilon,time_res = 1,dispatch = 1,delta_t=0.1,t_final=5,model=3,factor_daily_profile=0,prim_control_lim = 0.15*2*np.pi,prim_weight = 1)  \n",
    "  freq_synth_model_2 = omega_synth_model_2/(2*np.pi) + 50\n",
    "\n",
    "  synth_data_model_2[grid].append(freq_synth_model_2)\n",
    "  increments_model_2[grid].append(Increments(freq_synth_model_2,time_res = delta_t,step = 1))\n",
    "  autocor_model_2[grid].append(autocor(freq_synth_model_2,time_res = delta_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of too high frequency values:  0 Number of too low frequency values:  0\n",
      "Number of isolated peaks:  0\n",
      "Number of windows with constant frequency for longer than 15s:  0\n",
      "Number of Nan-intervals:  0\n",
      "Clean corrupted data ...\n"
     ]
    }
   ],
   "source": [
    "'''Model 3...'''\n",
    "synth_data_model_3 = {i:[]for i in grids}\n",
    "increments_model_3 = {i:[]for i in grids}\n",
    "autocor_model_3 = {i:[]for i in grids}\n",
    "p_3_model3, p_1_model3 = {i:[]for i in grids},{i:[]for i in grids}\n",
    "d_2_model3, d_0_model3 = {i:[]for i in grids},{i:[]for i in grids}\n",
    "'''adapt the parameter estimation to the particulat grids'''\n",
    "for grid in grids:\n",
    "\n",
    "  raw=pd.read_csv('./Data/Frequency_data_%s.csv'%(grid), sep=',')\n",
    "  freq = (raw[['Frequency']]/1000 +50).squeeze()\n",
    "  freq = data_cleaning(freq)\n",
    "  data = (freq-50)*(2*np.pi)   #Use the angular velocity for the calcualltions\n",
    "  \n",
    "  trend = 1\n",
    "  bw_drift = 0.1\n",
    "  bw_diff = 0.1\n",
    "  dist_drift = 1200\n",
    "  if grid == 'Balearic':\n",
    "      dist_diff = 350   #dist_drift = 1600 for smaller data set\n",
    "      dispatch = 1\n",
    "      prim_control_lim, prim_weight = 0.15*2*np.pi, 3 # set to 10 for small data set\n",
    "      Delta_P = power_mismatch(data,avg_for_each_hour = True,dispatch=1,start_minute=-2,end_minute=0,length_seconds_of_interval=5)\n",
    "  elif grid == 'Irish':\n",
    "      Delta_P = power_mismatch(data_filter(data,sigma = 6),avg_for_each_hour = True,dispatch=2,start_minute=0,end_minute=5,length_seconds_of_interval=5)\n",
    "      dispatch = 2\n",
    "      prim_control_lim, prim_weight = 0.13*2*np.pi, 3\n",
    "      dist_diff = 500 #300 \n",
    "  elif grid == 'Iceland':\n",
    "      dist_diff = 200 #300\n",
    "      dispatch = 0\n",
    "      prim_control_lim, prim_weight = 0, 1 #no additional control via HVDC transmission in the Iceland power grid\n",
    "      Delta_P = 0\n",
    "      trend = 0 # Represents a no-existing trend as there is no power dispatch schedule\n",
    "      \n",
    "  c_1 = KM_Coeff_1(data - trend * data_filter(data),dim= 1,time_res = 1,bandwidth = bw_drift,dist = dist_drift, order = 3)\n",
    "  c_2_decay = trend * exp_decay(data,time_res=1,size = 899)\n",
    "  epsilon = KM_Coeff_2(data - trend*data_filter(data), dim = 1, time_res = 1, bandwidth = bw_diff, dist = dist_diff, multiplicative_noise = True)\n",
    "  \n",
    "  (p_3_model3[grid], p_1_model3[grid]) = c_1 \n",
    "  (d_2_model3[grid], d_0_model3[grid]) = epsilon\n",
    "\n",
    "  \n",
    "  delta_t = 0.1\n",
    "  omega_synth_model_3 = Euler_Maruyama(data,c_1=c_1,c_2_decay=c_2_decay,Delta_P = Delta_P,epsilon=epsilon,time_res = 1,dispatch = dispatch,delta_t=delta_t,t_final=5,model=3,factor_daily_profile=0,prim_control_lim = prim_control_lim, prim_weight = prim_weight)\n",
    "  freq_synth_model_3 = omega_synth_model_3/(2*np.pi) + 50\n",
    "\n",
    "  synth_data_model_3[grid].append(freq_synth_model_3)\n",
    "  increments_model_3[grid].append(Increments(freq_synth_model_3,time_res = delta_t,step = 1))\n",
    "  autocor_model_3[grid].append(autocor(freq_synth_model_3,time_res = delta_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of too high frequency values:  0 Number of too low frequency values:  0\n",
      "Number of isolated peaks:  0\n",
      "Number of windows with constant frequency for longer than 15s:  0\n",
      "Number of Nan-intervals:  0\n",
      "Clean corrupted data ...\n"
     ]
    }
   ],
   "source": [
    "'''Model 4...'''\n",
    "synth_data_model_4 = {i:[]for i in grids}\n",
    "increments_model_4 = {i:[]for i in grids}\n",
    "autocor_model_4 = {i:[]for i in grids}\n",
    "'''adapt the parameter estimation to the particulat grids'''\n",
    "for grid in grids:\n",
    "  raw=pd.read_csv('./Data/Frequency_data_%s.csv'%(grid), sep=',')\n",
    "  freq = (raw[['Frequency']]/1000 +50).squeeze()\n",
    "  freq = data_cleaning(freq)\n",
    "  data = (freq-50)*(2*np.pi)   #Use the angular velocity for the calcualltions\n",
    "\n",
    "  trend = 1\n",
    "  bw_drift, bw_diff = 0.05, 0.05\n",
    "  if grid == 'Balearic':\n",
    "      dist_theta, dist_omega = 20,20\n",
    "      prim_control_lim, prim_weight = 0.13*2*np.pi, 3\n",
    "      factor_daily_profile = 2.5 # set to 1 for small data set\n",
    "  elif grid == 'Irish':\n",
    "      dist_theta, dist_omega = 15,15\n",
    "      prim_control_lim, prim_weight = 0.13*2*np.pi, 3\n",
    "      factor_daily_profile = 3.2\n",
    "  elif grid == 'Iceland':\n",
    "      dist_theta, dist_omega = 30,70    # choose larger intervals as the deviations in the grid are larger\n",
    "      prim_control_lim, prim_weight = 0, 1\n",
    "      factor_daily_profile = 0\n",
    "      trend = 1 #as we calculate witha 1-second resolution, we use also for Iceland a Gaussian filter (time window 60 seconds) as the 1-seconds resolution is too rough for the integration of the angular velocity \n",
    "\n",
    "  c_1 = KM_Coeff_1((data - trend*data_filter(data)),dim= 2,time_res = 1,bandwidth = bw_drift,dist = [dist_theta, dist_omega], order = 1)[0]\n",
    "  c_2 = KM_Coeff_1((data - trend*data_filter(data)),dim= 2,time_res = 1,bandwidth = bw_drift,dist = [dist_theta, dist_omega], order = 1)[1]\n",
    "  Delta_P = 0 \n",
    " \n",
    "  epsilon =   KM_Coeff_2(data - trend*data_filter(data), dim = 2, time_res = 1, bandwidth = bw_diff, dist = [dist_theta, dist_omega], multiplicative_noise = True)\n",
    "  delta_t = 0.1\n",
    "  omega_synth_model_4 = Euler_Maruyama(data,c_1=c_1,c_2_decay=c_2,Delta_P = Delta_P,epsilon=epsilon,time_res = 1,dispatch = 0,delta_t=delta_t,t_final=5,model=4,factor_daily_profile=factor_daily_profile,prim_control_lim = prim_control_lim, prim_weight = prim_weight)\n",
    "\n",
    "  freq_synth_model_4 = omega_synth_model_4/(2*np.pi) + 50\n",
    "\n",
    "  synth_data_model_4[grid].append(freq_synth_model_4)\n",
    "  increments_model_4[grid].append(Increments(freq_synth_model_4,time_res = delta_t,step = 1))\n",
    "  autocor_model_4[grid].append(autocor(freq_synth_model_4,time_res = delta_t))\n",
    "  \n",
    "  powers = np.array([[0,0],[1,0],[0,1],[1,1],[2,0],[0,2],[2,2]])\n",
    "  bins = np.array([300,300])\n",
    "  data_2d = np.array([integrate_omega(data - trend * data_filter(data),time_res=time_res,start_value = 0),data - trend * data_filter(data)]) #use theta as integrated omega\n",
    "  kmc_2d[grid], edges_2d[grid] = km(data_2d.transpose(),powers = powers,bins = bins,bw=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the calculated time series in the folder Create_figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grid in grids:\n",
    "\n",
    "    file_kmc  = str(os.getcwd().replace(os.sep, '/')) + '/Create_figures/%s_kmc'%(grid)   #data for figure 2\n",
    "    file_data = str(os.getcwd().replace(os.sep, '/')) + '/Create_figures/%s_data'%(grid)  #data for figures 3/4\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    np.savez_compressed(file_data,freq_origin = np.asarray(data_orig[grid]), freq_model1 = np.asarray(synth_data_model_1[grid]), \n",
    "                      freq_model2 = np.asarray(synth_data_model_2[grid]), freq_model3 = np.asarray(synth_data_model_3[grid]), freq_model4 = np.asarray(synth_data_model_4[grid]), \n",
    "                      incr_origin = np.asarray(increments_orig[grid]), incr_model1 = np.asarray(increments_model_1[grid]), incr_model2 = np.asarray(increments_model_2[grid]), \n",
    "                      incr_model3 = np.asarray(increments_model_3[grid]), incr_model4 = np.asarray(increments_model_4[grid]),  \n",
    "                      auto_origin = np.asarray(autocor_orig[grid]), auto_model1 = np.asarray(autocor_model_1[grid]), auto_model2 = np.asarray(autocor_model_2[grid]), \n",
    "                      auto_model3 = np.asarray(autocor_model_3[grid]), auto_model4 = np.asarray(autocor_model_4[grid]))\n",
    "\n",
    "\n",
    "    np.savez_compressed(file_kmc,\n",
    "        edges_1d = edges_1d[grid],\n",
    "        drift_1d = drift_1d[grid],\n",
    "        diffusion_1d = diffusion_1d[grid],\n",
    "        c_1_model1 = c_1_model1[grid],\n",
    "        c_1_model2 = c_1_model2[grid],\n",
    "        p_1_model3 = p_1_model3[grid],\n",
    "        p_3_model3 = p_3_model3[grid],\n",
    "        epsilon_model1 = epsilon_model1[grid],\n",
    "        epsilon_model2 = epsilon_model2[grid],\n",
    "        d_0_model3 = d_0_model3[grid],\n",
    "        d_2_model3 = d_2_model3[grid],\n",
    "        edges_2d = edges_2d[grid],\n",
    "        kmc_2d = kmc_2d[grid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piml_loc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
